{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Version Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import requests\n",
    "import pandas\n",
    "import matplotlib\n",
    "import numpy\n",
    "import pytz\n",
    "import pandas_market_calendars\n",
    "import statsmodels.api\n",
    "import sklearn\n",
    "\n",
    "print(\"Python version:\", sys.version)\n",
    "\n",
    "print(\"\\nrequests version:\", requests.__version__)\n",
    "print(\"pandas version:\", pandas.__version__)\n",
    "print(\"matplotlib version:\", matplotlib.__version__)\n",
    "print(\"numpy version:\", numpy.__version__)\n",
    "print(\"pytz version:\", pytz.__version__)\n",
    "print(\"pandas_market_calendars version:\", pandas_market_calendars.__version__)\n",
    "print(\"statsmodels version:\", statsmodels.__version__)\n",
    "print(\"sklearn version:\", sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pytz import timezone\n",
    "import pandas_market_calendars as mcal\n",
    "import statsmodels.api as sm\n",
    "from sklearn.cluster import KMeans\n",
    "from Code.Utility.API_CREDENTIALS import POLYGON_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.dpi'] = 150 # default DPI is 100, lower if plots are taking way too long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Calculating n trading days ago\n",
    "In this analysis, we will only be looking at the past 30 trading days. Choosing this over, say 252 trading days, offers us more recent changes in volatility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_back = 30\n",
    "\n",
    "nyse = mcal.get_calendar('NYSE') # NYSE calendar\n",
    "today = pd.Timestamp.today() # today's date for specific format for the code below\n",
    "start_date = today - pd.DateOffset(months=3) # Go back 3 months to be safe\n",
    "\n",
    "schedule = nyse.schedule(start_date=start_date, end_date=today)\n",
    "\n",
    "# Check if today is a weekend\n",
    "if today.weekday() >= 5: # This means if it's either Saturday (5) or Sunday (6)\n",
    "    last_n_trading_dates = mcal.date_range(schedule, frequency='1D').date[-days_back:] # get n days back\n",
    "else:\n",
    "    last_n_trading_dates = mcal.date_range(schedule, frequency='1D').date[-1-days_back:] # get n days back\n",
    "\n",
    "date_n_days_ago = last_n_trading_dates[0]\n",
    "today_date = today.date() # now we can just use the YYYY-MM-DD format\n",
    "\n",
    "print(f\"Today's date: {today_date}\")\n",
    "print(f\"{days_back} Trading days ago: {date_n_days_ago}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Pulling data from Polygon.io API\n",
    "Polygon.io's API has a free tier that you offers up to two years worth of historical data. The only downside is that you cannot make large requests, but since we are only looking at 30 days worth of data, that's not an issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pull_request_url = f\"https://api.polygon.io/v2/aggs/ticker/SPY/range/30/minute/{date_n_days_ago}/{today_date}?adjusted=true&sort=asc&limit=50000&apiKey={POLYGON_API_KEY}\"\n",
    "response = requests.get(pull_request_url)\n",
    "if response.status_code == 200: # success status code\n",
    "    data_json = response.json()\n",
    "else:\n",
    "    print(\"Request failed with status code:\", response.status_code)\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.DataFrame(data_json[\"results\"])\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Getting cash session data only\n",
    "As a futures trader and like most other traders, we thrive during peak volatility hours due to ease to capturing the trend. It is nice to know during which hours markets tend to be choppy. In this example, we will only be looking at the cash session of SPY (which is useful for ES derivatives traders (me) and SPY/SPX options traders)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = pd.to_datetime('8:30:00').time() # cash session open\n",
    "end_time = pd.to_datetime('14:30:00').time() # cash session close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cash_session_df = data_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Convert from Unix Timestamp to regular time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "central = timezone('America/Chicago') # Define UTC and Central Time timezones\n",
    "\n",
    "cash_session_df['Time'] = pd.to_datetime(cash_session_df['t'], unit='ms', utc=True) # Convert Unix timestamps to datetime in UTC\n",
    "\n",
    "cash_session_df['Time'] = cash_session_df['Time'].dt.tz_convert(central) # Convert UTC datetime to Central Time\n",
    "\n",
    "cash_session_df['Time'] = cash_session_df['Time'].dt.time # If you want only the time part in CT\n",
    "\n",
    "cash_session_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Filter time for cash session hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cash_session_df = cash_session_df[(cash_session_df['Time'] >= start_time) & (cash_session_df['Time'] <= end_time)] # filter only cash session\n",
    "cash_session_df = cash_session_df.assign(Range=np.log(cash_session_df[\"h\"]) - np.log(cash_session_df[\"l\"])) # taking log diff to make data stationary\n",
    "cash_session_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_time_df = cash_session_df.copy()\n",
    "range_time_df = range_time_df[[\"Time\", \"Range\"]] # reduce the dataframe to what we need\n",
    "range_time_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Bundle time intervals up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_grouped_df = range_time_df.groupby(\"Time\")[\"Range\"].apply(list) # bundle all the 30min intervals together\n",
    "time_grouped_df.head(13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRE-BOOTSTRAP\n",
    "Our data does not appear to come from any specific distribution, which makes visualizing and interpreting the data less simple. As you can see, the histograms are lackluster, and doing an analysis just on this would have too many holes in the data. In the next step, we will perform a bootstrap sampling technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Times = ['08:30:00', '09:00:00', '09:30:00', '10:00:00', '10:30:00', '11:00:00', '11:30:00', '12:00:00', '12:30:00', '13:00:00', '13:30:00', '14:00:00', '14:30:00']\n",
    "colors = plt.cm.tab20(np.linspace(0, 1, 13)) # colors of histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Pre-Bootstrap Histograms of Each Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(time_grouped_df.shape[0]):\n",
    "    plt.hist(time_grouped_df[i], bins='fd', histtype='step', fill=False, alpha=1, color=colors[i])\n",
    "    plt.title(f\"Distribution of {Times[i]}\")\n",
    "    plt.xlabel(\"Log Difference of High and Low\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOOTSTRAP TIME\n",
    "The idea behind bootstrapping is simple: use your sample as a proxy for the population, so sample from it with replacement so simulate different samples. This approach makes little assumptions, especially about the distribution of the data, so it is a robust technique. I am bootstrap sampling from each time and calculating the mean of that sample. Due to the Central Limit Theorem, the means will converge to a Gaussian distribution. That makes visualizing and interpreting the data MUCH easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10000000 # times we will bootstrap sample from our data\n",
    "resampled_data = []\n",
    "time_mean = np.array([])\n",
    "time_stddev = np.array([])\n",
    "\n",
    "for time, data_points in time_grouped_df.items():\n",
    "    bootstrap_means = []\n",
    "\n",
    "    bootstrap_samples = np.random.choice(data_points, size=(n_samples, len(data_points)), replace=True) # bootstrap\n",
    "    sample_mean = np.mean(bootstrap_samples, axis=1)\n",
    "\n",
    "    time_mean = np.append(time_mean, np.mean(bootstrap_samples))\n",
    "    time_stddev = np.append(time_stddev, np.std(bootstrap_samples))\n",
    "\n",
    "    bootstrap_means.append(sample_mean)\n",
    "    resampled_data.append((time, bootstrap_means))\n",
    "\n",
    "bootstrapped_time_grouped_df = pd.DataFrame(resampled_data, columns=['Time', 'ResampledData']) # dataframe with the bootstrapped data\n",
    "bootstrapped_time_grouped_df.head(13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Q-Q Plot Test for Normality\n",
    "<center>\n",
    "A Q-Q plot (Quantile-Quantile plot) is a graphical tool used to assess whether a dataset follows a specific theoretical distribution, such as the normal distribution. It compares the quantiles of the observed data against the quantiles expected from the theoretical distribution, and if the points lie approximately along a straight line, it suggests that the data is consistent with the assumed distribution.\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(bootstrapped_time_grouped_df.shape[0]):\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    sm.qqplot(bootstrapped_time_grouped_df.iloc[i,1][0][:10000], line='s', fit=True)\n",
    "\n",
    "    plt.title(f\"Q-Q Plot for {Times[i]}\")\n",
    "    plt.xlabel(\"Theoretical Quantiles\")\n",
    "    plt.ylabel(\"Sample Quantiles\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Notice, some Q-Q plots display slight skewness that we can observe in the histograms and box plots below. An important detail to remember is important financial or economic data is typically released 11am - 1pm CT. That heavily contributes to fatter distribution tails. However, for this analysis, we can assume normality for all the distributions due to the majority of our data being normally distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Histograms of the bootstrapped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 12))\n",
    "for i in range(bootstrapped_time_grouped_df.shape[0]):\n",
    "    plt.hist(bootstrapped_time_grouped_df.iloc[i,1], bins='fd', label=f\"{Times[i]}\", histtype='step', alpha=1, color=colors[i])\n",
    "    plt.axvline(x=time_mean[i], color=colors[i])\n",
    "\n",
    "plt.title(f\"Distributions of Bootstrap Means For Each Interval\\nMean of distribution displayed by vertical line\")\n",
    "plt.xlabel(\"Log Difference of High and Low\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Box Plot of the bootstrapped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "boxplot = plt.boxplot(bootstrapped_time_grouped_df[\"ResampledData\"], labels=bootstrapped_time_grouped_df[\"Time\"], patch_artist=True, boxprops=dict(facecolor='gray'), showfliers=False, whis=(2.5, 97.5))\n",
    "\n",
    "for patch, color in zip(boxplot['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "plt.title(\"Bootstrap Means for Log Difference of high - low\\nNYSE trading times, binned at every 30 minutes. Box Plot of Bootstrap Means For Each Interval\\nWhiskers include ±2 standard deviations\")\n",
    "plt.xlabel(\"Time Intervals\")\n",
    "plt.ylabel(\"Log Difference of High and Low\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Rankings of the 30-minute intervals\n",
    "The scores are ranked from highest (most volatility) to lowest (least volatility). The scores are mostly weighted by their mean but are punished by a larger coefficient of variation (mean/stddev ratio). The Coefficient of variation only is significant when the intervals are lower ranked. The intuition behind this ranking metric is that we want to be confident that, on average, the volatility within that interval is true to its average (due to punishing the interval for having a wider variance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CoeffOfVar = (time_stddev / time_mean)*10\n",
    "scaled_time_mean = time_mean * 1000\n",
    "\n",
    "data_score = scaled_time_mean**4 - CoeffOfVar*2 # custom ranking metric...prioritize mean but punish by larger standard deviations\n",
    "\n",
    "data = list(zip(Times, data_score)) # combine Time labels and the scores\n",
    "sorted_data = sorted(data, key=lambda x: x[1], reverse=True) # sort from highest to lowest\n",
    "\n",
    "print(\"Time\\t   Score\")\n",
    "for time, score in sorted_data:\n",
    "    print(f\"{time} - {score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "reshaped_scores = np.array(data_score).reshape(-1, 1)\n",
    "\n",
    "# Apply k-means clustering\n",
    "kmeans = KMeans(n_clusters=4, n_init='auto', max_iter=1000000, tol=1e-8, random_state=0).fit(reshaped_scores)\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# Sort centroids and create a stable mapping\n",
    "sorted_centroids_indices = np.argsort(centroids, axis=0).flatten()\n",
    "category_map = {sorted_centroids_indices[0]: \"Low\",\n",
    "                sorted_centroids_indices[1]: \"Medium\",\n",
    "                sorted_centroids_indices[2]: \"High\",\n",
    "                sorted_centroids_indices[3]: \"Very High\"}\n",
    "\n",
    "# Apply this mapping to your labels\n",
    "volatility_category = [category_map[label] for label in kmeans.labels_]\n",
    "category_colors = {\"Very High\": \"green\", \"High\": \"lightgreen\", \"Medium\": \"orange\", \"Low\": \"red\"}\n",
    "\n",
    "# Scatter plot of data points with 4 categories\n",
    "plt.figure(figsize=(20, 6))\n",
    "for label in np.unique(kmeans.labels_):\n",
    "    plt.scatter(reshaped_scores[kmeans.labels_ == label, 0], \n",
    "                np.zeros_like(reshaped_scores[kmeans.labels_ == label, 0]), \n",
    "                color=category_colors[category_map[label]], \n",
    "                label=category_map[label], \n",
    "                alpha=0.6)\n",
    "\n",
    "# Plotting centroids\n",
    "for i, centroid in enumerate(centroids):\n",
    "    plt.scatter(centroid, 0, color='black', marker='x', s=100)\n",
    "    plt.text(centroid, 0.02, f'Centroid {i}', horizontalalignment='center')\n",
    "\n",
    "plt.title(\"K-Means Clustering of Interval Scores with 4 Categories\")\n",
    "plt.xlabel(\"Interval Score\")\n",
    "plt.yticks([])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Boxplot with colored categories for 4 categories\n",
    "plt.figure(figsize=(20, 12))\n",
    "boxplot = plt.boxplot(bootstrapped_time_grouped_df[\"ResampledData\"], labels=Times, patch_artist=True, showfliers=False, whis=(2.5, 97.5))\n",
    "\n",
    "for patch, category in zip(boxplot['boxes'], volatility_category):\n",
    "    patch.set_facecolor(category_colors[category])\n",
    "    patch.set_alpha(1)\n",
    "\n",
    "plt.title(\"Bootstrap Means for Log Difference of high - low\\nNYSE trading times, binned at every 30 minutes\\nWith 4 Volatility Categories\")\n",
    "plt.xlabel(\"Time Intervals\")\n",
    "plt.ylabel(\"Log Difference of High and Low\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Decay of scores\n",
    "For visual purposes, let's see what decay of the sorted scores. This will help visualize the rapid decay of volatility as the intervals are lower ranked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_scores = [item[1] for item in sorted_data]\n",
    "plt.plot(sorted_scores)\n",
    "plt.title(\"Decay of Scores\")\n",
    "plt.ylabel(\"Scores\")\n",
    "plt.xlabel(\"Ranking\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Last updated: October 30, 2023"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
